In the spirit of open data, I have decided to embrace radical transparency and document my development process in detail. I'll send you the whole git repository, rather than just the end result, so you can see the timeline and any stalls and missteps along the way

I'm going to treat the three-hour suggestion as a deadline, though I'll extend the deadline rather than sending you something broken if it comes down to a choice between the two.

A couple of days ago I had a look at the available sources of data on local government spending. It seemed clear that Chris Taggart has done a considerable job collating the data from different sources and putting them into a common format, and that it would be foolish to try and redo this work (on a tight deadline) unless evidence emerges of serious problems with Chris's data. On the other hand, the JSON/XML interface on Openly Local is apparently ill-suited for the sorts of queries we will need. So our preliminary decision -- which may need to be revisited if this approach proves troublesome -- is to use the downloadable data file from http://openlylocal.com/councils/spending.csv.zip and process it into queriable form locally.

The only other preparation I've already done is to register the Twitter username @theyspentwhat, and to register a Twitter application of the same name. This account and application will be used for the Twitter integration.

It is twenty past nine on the evening of Wednesday, 2 March 2011. I'm about to check this file into git, and start the timer. The remainder of this file will consist of time-stamped entries as design decisions are made or problems arise.

[Wed Mar  2 21:25:07 GMT 2011]
I note that the author name appears in the git commit log, so if you wish to review this without discovering my identity you will need to redact that. Perhaps egrep -v '^Author: ' would do it?

[Wed Mar  2 21:26:57 GMT 2011]
Let's try a bit of README-driven development. I'll start a README.txt file documenting how this (presently imaginary) piece of software works, and then aim for convergence of the description and the reality –– mostly by modifying the reality, but tweaking the description where necessary as well.

[Wed Mar  2 21:37:17 GMT 2011]
Start with a description of the basic information flow. The advantages of this design, compared with a single monolithic process, are:
 - It makes it easy to test the core function, represented by the "theyspentwhat" process, without using Twitter or needing to mock the Twitter APIs.
 - The Twitter-related helper processes seem likely to be generally useful, for other projects.

[Wed Mar  2 21:48:12 GMT 2011]
Let's try and find the most interesting interpretation of every query, if the query is ambiguous. In the context of spending data, "most interesting" ought to be approximable by "involving the largest amount of money".

