In the spirit of open data, I have decided to embrace radical transparency and document my development process in detail. I'll send you the whole git repository, rather than just the end result, so you can see the timeline and any stalls and missteps along the way

I'm going to treat the three-hour suggestion as a deadline, though I'll extend the deadline rather than sending you something broken if it comes down to a choice between the two.

A couple of days ago I had a look at the available sources of data on local government spending. It seemed clear that Chris Taggart has done a considerable job collating the data from different sources and putting them into a common format, and that it would be foolish to try and redo this work (on a tight deadline) unless evidence emerges of serious problems with Chris's data. On the other hand, the JSON/XML interface on Openly Local is apparently ill-suited for the sorts of queries we will need. So our preliminary decision -- which may need to be revisited if this approach proves troublesome -- is to use the downloadable data file from http://openlylocal.com/councils/spending.csv.zip and process it into queriable form locally.

The only other preparation I've already done is to register the Twitter username @theyspentwhat, and to register a Twitter application of the same name. This account and application will be used for the Twitter integration.

It is twenty past nine on the evening of Wednesday, 2 March 2011. I'm about to check this file into git, and start the timer. The remainder of this file will consist of time-stamped entries as design decisions are made or problems arise.

[Wed Mar  2 21:25:07 GMT 2011]
I note that the author name appears in the git commit log, so if you wish to review this without discovering my identity you will need to redact that. Perhaps egrep -v '^Author: ' would do it?

[Wed Mar  2 21:26:57 GMT 2011]
Let's try a bit of README-driven development. I'll start a README.txt file documenting how this (presently imaginary) piece of software works, and then aim for convergence of the description and the reality –– mostly by modifying the reality, but tweaking the description where necessary as well.

[Wed Mar  2 21:37:17 GMT 2011]
Start with a description of the basic information flow. The advantages of this design, compared with a single monolithic process, are:
 - It makes it easy to test the core function, represented by the "theyspentwhat" process, without using Twitter or needing to mock the Twitter APIs.
 - The Twitter-related helper processes seem likely to be generally useful, for other projects.

[Wed Mar  2 21:48:12 GMT 2011]
Let's try and find the most interesting interpretation of every query, if the query is ambiguous. In the context of spending data, "most interesting" ought to be approximable by "involving the largest amount of money".

[Wed Mar  2 22:04:13 GMT 2011]
First pass description of tweet syntax. Note that both the names given in this example are partial: the official name of the council is "London Borough of Islington", and the company is called "Alphatrack Systems Limited". This much flexibility is a sine qua non: we can't expect anyone to use the full names if the system is to be even remotely usable. The syntax should be as fluid as possible, so ideally queries like "Islington Alphatrack" or "Alphatrack Islington" should also work, triggered by the fact that one of the words is the name of a council and the other is the name of a company.

I'm still attracted to the vague idea that ambiguous queries should be resolved to the most interesting of their possible interpretations, but it may be unrealistic to attempt something like that on such a tight deadline. Definitely a refinement to be added once a simpler version is working, anyway!

[Wed Mar  2 22:10:54 GMT 2011]
Okay, about 45 minutes have elapsed. It's probably time to write some code. Let's start with one part that could prove troublesome: pulling the data into a queriable format. The raw uncompressed CSV file is < 1GB, so it's perfectly reasonable to think about keeping the whole data set in RAM. On that basis the first thing I'll try is to pull the data into Redis, which ought at least to be nice and fast. Let's say we'll bucket the spending by council, supplier, and month.

[Wed Mar  2 22:54:00 GMT 2011]
An observation: The Openly Local data sometimes use different supplier IDs for the same company. For example, BT appear as:

 - http://openlylocal.com/suppliers/16196
 - http://openlylocal.com/suppliers/143834
 - http://openlylocal.com/suppliers/196367
 - http://openlylocal.com/suppliers/212112
 - http://openlylocal.com/suppliers/212113
 - http://openlylocal.com/suppliers/212666
 - http://openlylocal.com/suppliers/239038

probably among others. Some but not all of these can be related to the same company by the "payee" information. For example http://openlylocal.com/suppliers/196367.json and http://openlylocal.com/suppliers/212113.json have payee_id = 60.

The CSV file we're using has a field called "payee_resource_uri", which could be used to correlate suppliers. However it is often blank.

For now this is just a caveat.

[Wed Mar  2 23:37:05 GMT 2011]
First draft data import script, which at least approximately works.

(I'm imagining that the final version will use sorted sets, so we can pull out the largest transaction of a particular type. This version doesn't.)

It doesn't look as though I'll have anything very good within the three hour limit, at this rate. I'll see if I can round out the complete system in a sketchy way in the next forty minutes, and come back to it tomorrow evening. The downside of radical transparency: I can't pretend this never happened...

[Thu Mar  3 00:09:29 GMT 2011]
The (self-imposed) three-hour deadline has defeated me.

I tried to find a nice open-source library that supports the Twitter streaming API, since I thought that was the only way I could manage it quickly enough. First I tried https://github.com/hoisie/twitterstream (for the Go language), but that requires https://github.com/hoisie/httplib.go, which in turn will not build with the current version of Go.

Then I looked for Python libraries (http://dev.twitter.com/pages/libraries#python), but none of them have documented support for the streaming API, that I could find.

So I'll do it the old-fashioned way -- which is probably what I ought to have done in the first place -- and then return to this tomorrow.

[Thu Mar  3 21:15:01 GMT 2011]
My biggest mistake yesterday was to rush in the hope of meeting the self-imposed three-hour deadline. I should have abandoned it as soon as it became obvious I was not going to meet it, rather than starting to make poor decisions in a hurry.

Having just had a calmer look, it's clear that the Tweepy library for Python is an excellent piece of software that does everything we need. I should have looked at the source code before, rather than skimming the online documentation: the code is beautiful. So that's the Twitter integration needs sorted. Now for the fun part!

